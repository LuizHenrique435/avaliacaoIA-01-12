{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLN com Embedding em VectorDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Bibliotecas para PLN \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m  \n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Para chunks e embeddings \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter \u001b[38;5;66;03m# pip install langchain-text-splitters\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer \u001b[38;5;66;03m# pip install sentence_transformers \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Para leitura de PDF \u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/ML/venv/lib/python3.10/site-packages/langchain_text_splitters/__init__.py:19\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Language,\n\u001b[1;32m     10\u001b[0m     TextSplitter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     split_text_on_tokens,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcharacter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     CharacterTextSplitter,\n\u001b[1;32m     17\u001b[0m     RecursiveCharacterTextSplitter,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     ElementType,\n\u001b[1;32m     21\u001b[0m     HTMLHeaderTextSplitter,\n\u001b[1;32m     22\u001b[0m     HTMLSectionSplitter,\n\u001b[1;32m     23\u001b[0m     HTMLSemanticPreservingSplitter,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveJsonSplitter\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JSFrameworkTextSplitter\n",
      "File \u001b[0;32m~/Downloads/ML/venv/lib/python3.10/site-packages/langchain_text_splitters/html.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbs4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResultSet\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     _HAS_NLTK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/ML/venv/lib/python3.10/site-packages/nltk/__init__.py:180\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Check if tkinter exists without importing it to avoid crashes after\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# forks on macOS. Only nltk.app, nltk.draw, and demo modules should\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# have top-level tkinter imports. See #2949 for more details.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtkinter\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Downloads/ML/venv/lib/python3.10/site-packages/nltk/downloader.py:2528\u001b[0m\n\u001b[1;32m   2518\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2526\u001b[0m \n\u001b[1;32m   2527\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[0;32m-> 2528\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m \u001b[43mDownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2529\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[0;32m~/Downloads/ML/venv/lib/python3.10/site-packages/nltk/downloader.py:503\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_download_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/ML/venv/lib/python3.10/site-packages/nltk/downloader.py:1056\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[0;32m-> 1056\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mpath:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Bibliotecas para PLN \n",
    "\n",
    " \n",
    "\n",
    "# Para chunks e embeddings \n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # pip install langchain-text-splitters\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers \n",
    "\n",
    " \n",
    "\n",
    "# Para leitura de PDF \n",
    "\n",
    "import pdfplumber # pip install pdfplumber \n",
    "\n",
    " \n",
    "\n",
    "# Para tratamento de texto \n",
    "\n",
    "import re \n",
    "\n",
    "import spacy # python -m spacy download pt_core_news_sm \n",
    "\n",
    "import nltk # pip install nltk \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Baixando dados do NLTK necessários (se ainda não tiver) \n",
    "\n",
    "nltk.download('stopwords') # rodar apenas uma vez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas para Banco de Dados Vetorial (Vector Database) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb \n",
    "\n",
    "from langchain_chroma import Chroma "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando PLN - Preparação do texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto em caracteres: 913\n",
      "Título: Introdução ao Machine Learning Machine learning é um campo da inteligência artificial que desenvolve algoritmos capazes de aprender padrões a partir de dados. Os principais tipos incluem aprendizado supervisionado, aprendizado não supervisionado e aprendizado por reforço. Aprendizado supervisionado envolve treinar um modelo com dados rotulados, como prever preços de casas com base em características como tamanho, localização e número de quartos. Aprendizado não supervisionado detecta padrões ocultos em dados não rotulados, como segmentação de clientes em marketing. Aprendizado por reforço ensina agentes a tomar decisões em ambientes dinâmicos para maximizar recompensas. Redes neurais profundas são usadas em visão computacional e processamento de linguagem natural. Modelos como CNNs e Transformers têm revolucionado tarefas como tradução automática, reconhecimento de imagens e geração de texto.\n"
     ]
    }
   ],
   "source": [
    "# Carregando PDFs \n",
    "\n",
    " \n",
    "\n",
    "def ler_pdf(caminho_pdf):\n",
    "    leitor_pdf = pdfplumber.open(caminho_pdf) \n",
    "\n",
    "# page = leitor_pdf.pages[0]\n",
    "    texto = \"\" \n",
    "\n",
    "    for pagina in range(len(leitor_pdf.pages)):\n",
    "        texto += leitor_pdf.pages[pagina].extract_text()\n",
    "    texto = texto.replace(\"\\n\", \" \") \n",
    "\n",
    "    return texto \n",
    " \n",
    "\n",
    "# Carregar os documentos do PDF \n",
    "\n",
    "arquivo_pdf = \"/Users/ALEKS/Downloads/ML/introducaoml.pdf\" \n",
    "\n",
    "texto_pdf = ler_pdf(arquivo_pdf) \n",
    "\n",
    " \n",
    "\n",
    "# Tamanho do texto \n",
    "\n",
    "print(\"Tamanho do texto em caracteres:\",len(texto_pdf)) \n",
    "\n",
    "\n",
    "# Arquivo PDF original \n",
    "\n",
    "print(texto_pdf) \n",
    "\n",
    "\n",
    "# PLN \n",
    "\n",
    "# Carregar o modelo de linguagem do spaCy \n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\") # Definir stopwords \n",
    "\n",
    "api_stop_words = set(stopwords.words('portuguese')) \n",
    "\n",
    "minhas_stop_words = {'a','e','i','o', 'u'} \n",
    "\n",
    "stop_words = api_stop_words | minhas_stop_words \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função para fazer o tratamento de linguagem natural usando spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto stop_words: 209 \n",
      "Stop_words ordenadas: \n",
      " ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'i', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tém', 'tínhamos', 'u', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos']\n"
     ]
    }
   ],
   "source": [
    "def tratamento_pln(texto):\n",
    "    texto = texto.lower() # 1. Normalização: Colocar o texto em minúsculas\n",
    "    \n",
    "    texto = re.sub(r'[^a-zA-Záéíóú\\s]', '', texto) # na expressão regular estão as exceções  2. Remoção de números, pontuações e caracteres especiais \n",
    "    \n",
    "    doc = nlp(texto) # 3. Tokenização com spaCy\n",
    "    \n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    clean_tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]  # 4. Remoção de stopwords, remoção de pontuação e Lematização (clean_tokens = tokens lematizados e sem stopwords)\n",
    "    \n",
    "    clean_text = ' '.join(clean_tokens) # 5. Juntar tokens lematizados de volta em uma string\n",
    "    \n",
    "    return clean_text \n",
    "\n",
    "    #return texto \n",
    "\n",
    "    # Visualizando as Stop Words \n",
    "\n",
    "print(\"Tamanho do conjunto stop_words:\",len(stop_words),\"\\nStop_words ordenadas: \\n\",sorted(list(stop_words))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando as Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto em caracteres: 913\n",
      "título introduo machine Learning machine learning campo inteligncia artificial desenvolver algorit capaz aprender padre partir dado principal tipo incluir aprendizar supervisionar aprendizar supervisionado aprendizar reforo aprendizado supervisionar envolver treinar modelo dado rotular prever preo casa base característica tamanho localizao número quarto aprendizar supervisionado detectar padre oculto dado rotulado segmentao cliente marketing aprendizar reforo ensina agente tomar decises ambiente dinmico maximizar recompenso rede neural profundo so usar viso computacional processamento linguagem natural modelo cnns transformer tm revolucionar tarefa traduo automático reconhecimento imagem gerao texto\n",
      "Tamanho do texto em caracteres: 708\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Chamada de PLN \n",
    "\n",
    "texto_pdf_tratado = tratamento_pln(texto_pdf) \n",
    "\n",
    " \n",
    "\n",
    "# Tamanho do texto \n",
    "\n",
    "print(\"Tamanho do texto em caracteres:\",len(texto_pdf)) \n",
    "\n",
    " \n",
    "\n",
    "# Arquivo PDF tratado \n",
    "\n",
    "print(texto_pdf_tratado) \n",
    "\n",
    " \n",
    "\n",
    "# Tamanho do texto tratado \n",
    "\n",
    "print(\"Tamanho do texto em caracteres:\",len(texto_pdf_tratado)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação do Texto para BD Vetorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RecursiveCharacterTextSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Dividindo os documentos em Chunks \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mRecursiveCharacterTextSplitter\u001b[49m(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \n\u001b[1;32m      5\u001b[0m chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(texto_pdf_tratado) \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunks, \u001b[38;5;28mlen\u001b[39m(chunks)) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'RecursiveCharacterTextSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "# Dividindo os documentos em Chunks \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30) \n",
    "\n",
    "chunks = text_splitter.split_text(texto_pdf_tratado) \n",
    "\n",
    "print(chunks, len(chunks)) \n",
    "\n",
    " \n",
    "\n",
    "# Carregar o modelo de Embeddings bem como gerar os Embeddings \n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "embeddings = model.encode(chunks) \n",
    "\n",
    " \n",
    "\n",
    "# Gerando IDs automaticamente \n",
    "\n",
    "uids = [f\"doc_{i}\" for i in range(len(chunks))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação do ChromaDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['doc_1']], 'embeddings': None, 'documents': [['partir dado principal tipo incluir aprendizar supervisionar aprendizar supervisionado aprendizar reforo aprendizado supervisionar envolver treinar']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None]], 'distances': [[1.1930792331695557]]}\n",
      "ID: doc_1\n",
      "Distância: 1.1930792331695557\n",
      "Documento: partir dado principal tipo incluir aprendizar supervisionar aprendizar supervisionado aprendizar reforo aprendizado supervisionar envolver treinar\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Criar o banco de dados \n",
    "\n",
    "client = chromadb.Client() \n",
    "\n",
    "collection = client.create_collection(name=\"machlrn\") \n",
    "\n",
    " \n",
    "\n",
    "#client.delete_collection(\"machlrn\") \n",
    "\n",
    " \n",
    "\n",
    "# Adicionar os documentos ao banco de dados \n",
    "\n",
    "collection.add(documents=chunks, embeddings=embeddings, ids=uids) \n",
    "\n",
    " \n",
    "\n",
    "# Realizar a busca usando collection.query \n",
    "\n",
    "query_embedding = model.encode([\"O que significa agir racionalmente?\"]) \n",
    "\n",
    "query_embedding = model.encode([\"Em 1965, que programas já existiam?\"]) \n",
    " \n",
    " \n",
    "\n",
    "results = collection.query(query_embeddings=query_embedding, n_results=1) \n",
    "\n",
    "print(results) \n",
    "\n",
    " \n",
    "\n",
    "# Imprimir os resultados \n",
    "\n",
    "# Fazendo a varredura sobre os campos 'ids', 'distances' e 'documents' \n",
    "\n",
    "for i in range(len(results['ids'][0])):\n",
    "    doc_id = results['ids'][0][i]\n",
    "    \n",
    "    distance = results['distances'][0][i]\n",
    "    \n",
    "    document = results['documents'][0][i] \n",
    "\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    \n",
    "    print(f\"Distância: {distance}\")\n",
    "    \n",
    "    print(f\"Documento: {document}\")\n",
    "    \n",
    "    print(\"-\" * 40) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
