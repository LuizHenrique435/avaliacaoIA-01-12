{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Felipe\n",
      "[nltk_data]     Pereira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # pip install langchain-text-splitters\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers\n",
    "\n",
    "import pdfplumber # pip install pdfplumber \n",
    "\n",
    " \n",
    "# Para tratamento de texto \n",
    "\n",
    "import re \n",
    "\n",
    "import spacy # python -m spacy download pt_core_news_sm \n",
    "\n",
    "import nltk # pip install nltk \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Download das stop words\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import chromadb \n",
    "\n",
    "from langchain_community.vectorstores import Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb funcionando!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "print(\"chromadb funcionando!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLN (Processamento de linguagem natural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Leitura do PDF e contagem de caracteres e linhas do texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto em caracteres: 929\n",
      "Número total de linhas: 13\n",
      "Os cientistas estavam analisando grandes volumes de dados para descobrir padrões que pudessem prever o comportamento humano. Embora os resultados iniciais fossem promissores, ainda havia muito a ser feito. Alguns algoritmos aprenderam rapidamente, enquanto outros exigiram ajustes finos e reprocessamento de informações. A equipe percebeu que o contexto desempenhava um papel crucial na interpretação dos dados coletados. Durante os testes, surgiram desafios inesperados relacionados à ambiguidade linguística e à qualidade das fontes utilizadas. Mesmo frases simples podiam assumir significados diferentes dependendo do domínio em que eram aplicadas. Ao final da primeira fase do projeto, os pesquisadores estavam confiantes de que as técnicas de inteligência artificial estavam evoluindo, mas também conscientes das limitações atuais. Era necessário combinar abordagens estatísticas com conhecimento linguístico mais profundo. \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Função para ler o pdf\n",
    "def ler_pdf(caminho_pdf):\n",
    "    leitor_pdf = pdfplumber.open(caminho_pdf)  # abre o pdf\n",
    "    texto = \"\"  # string vazia para guardar o texto\n",
    "\n",
    "    # passa por todas as páginas do pdf\n",
    "    for pagina in range(len(leitor_pdf.pages)):\n",
    "        pagina_texto = leitor_pdf.pages[pagina].extract_text()\n",
    "        if pagina_texto:\n",
    "            texto += pagina_texto + \"\\n\"   # mantemos a quebra de linha\n",
    "\n",
    "    # contar quantas linhas tem antes de remover \\n\n",
    "    numero_linhas = len(texto.splitlines())\n",
    "\n",
    "    # agora removemos quebras de linha, se necessário\n",
    "    texto = texto.replace(\"\\n\", \" \")\n",
    "\n",
    "    return texto, numero_linhas\n",
    "\n",
    "\n",
    "# Carregar os documentos do PDF\n",
    "arquivo_pdf = r\"C:\\Users\\Felipe Pereira\\Downloads\\comportamento.pdf\"\n",
    "texto_pdf, total_linha = ler_pdf(arquivo_pdf)  # chama a função e armazena o texto\n",
    "\n",
    "# Tamanho do texto\n",
    "print(\"Tamanho do texto em caracteres:\", len(texto_pdf))\n",
    "\n",
    "# Número de linhas do PDF\n",
    "print(\"Número total de linhas:\", total_linha)\n",
    "\n",
    "# Arquivo PDF original\n",
    "print(texto_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 2.B\n",
    "\n",
    "O tamanho do texto em caracteres é de 929\n",
    "O número de linhas que contem no texto é 13\n",
    "\n",
    "# Questão 2.C\n",
    "\n",
    "Como estou usando tamanho pequeno de 200 caracteres para o chunk, meu objetivo é aumentar o número de palavras chaves em um chunk, se eu deixasse as stopwords elas ocupariam um espaço grande sendo que são palavras de baixo valor semântico.\n",
    "Escolhas das minhas stopwords: estou buscando remover artigos frequentes que aparecem no texto e vogais como u e i que não tem muito valor semântico. Assim busco eliminar o máximo de ruído que posso.\n",
    "\n",
    "# Questão 2.D\n",
    "\n",
    "Transformo o texto todo em mínusculo pois a letra maíuscula é um ruído. Exemplo disso: se eu tiver em um mesmo documento as palavras Erro e erro, elas viraram dois vetores diferentes, so que essa palavra tem o mesmo valor semântico então a transformação do texto para minusculo é extremamente importante para a normalização, visto que não sobe para o VectorDB vetores com o mesmo significado.\n",
    "\n",
    "# Questão 2.E\n",
    "\n",
    "Preferi usar a lemmatização do que a stemização porque por meio da lemmatização eu consigo manter o significado semântico da palavra. Além disso, busco manter o significado porque meu documento é sobre o assunto de Inteligência Artificial e cada palavra é importante para o contexto e entendimento do texto. Demonstro isso com exemplos abaixo:\n",
    "\n",
    "    Stematização: rapidez importa\n",
    "        [\"correndo\", \"correção\"] -> [\"corr\"]\n",
    "        Perde o significado (é bastante usado para analise de documentos com infos rapidas, calculos)\n",
    "        A partir do exemplo, consigo analisar que o modelo não saberia diferenciar correndo e correção.\n",
    "\n",
    "    Lemmatização: contexto do texto importa\n",
    "        [\"correndo\", \"correção\"] -> [\"correr\", \"correção\"] sei o significado\n",
    "        Ela transforma substantivos para o singular, verbos para sua forma no infinitivo e etc.\n",
    "Gerar\n",
    "\n",
    "Optei por deixar os acentos e os números, pois a remoção dos acentos pode perder significado da palavra, visto que a língua portuguesa utiliza muito. Com a remoção do acento, posso mudar completamente o conceito que o vetor representa. Também vi que no meu documento os números são importantes pois referenciam datas (anos). Além disso estou usando um modelo moderno de embedding (model = Sentence Transformer('all-MiniLM-L6-v2')), o qual já é capaz de entender essa letras e acentuações do português.\n",
    "\n",
    "Na parte de tokenização com spacy, uso a tokenização por palavra. Isso é uma prática de extrema importância, pois para eu saber se uma palavra é stopword ou não eu não posso comparar com um texto inteiro, tem que ser por palavras. Ademais, faço a lemmatização também, que consiste em transformar uma palavra na sua forma mais primitiva que existe (ex: correndo -> correr), por isso da tokenização por palavra.\n",
    "\n",
    "    - Essa etapa de pré-processamento é diferente da tokenização por sub-palavra, que o próprio modelo de embedding usa internamente para gerar os vetores.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto stop_words: 209\n",
      "\n",
      "Stop_words ordenadas:\n",
      " ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'i', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tém', 'tínhamos', 'u', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos']\n",
      "Tamanho do texto original em caracteres: 929\n",
      "\n",
      "TEXTO TRATADO:\n",
      "\n",
      "cientista analisar grande volume dado descobrir padrão poder prever comportamento humano embora resultado inicial promissor ainda haver fazer algum algoritmos aprender rapidamente enquanto outro exigir ajuste fino reprocessamento informação equipe perceber contexto desempenhar papel crucial interpretação dado coletar durante teste surgir desafio inesperado relacionar   ambiguidade linguístico   qualidade fonte utilizar frase simples poder assumir significado diferente depender domínio aplicar final primeiro fase projeto pesquisador confiante técnica inteligência artificial evoluir consciente limitação atual necessário combinar abordagem estatístico conhecimento linguístico profundo\n",
      "\n",
      "Tamanho do texto tratado em caracteres: 690\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Carregar o modelo de linguagem do spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# definir stopwords\n",
    "api_stop_words = set(stopwords.words('portuguese'))  # carrega as stopwords padrões\n",
    "minhas_stop_words = {'a', 'e', 'i', 'o', 'u'}        # stopwords personalizadas\n",
    "stop_words = api_stop_words | minhas_stop_words      # une as stopwords\n",
    "\n",
    "# Função para fazer o tratamento de linguagem natural usando spacy\n",
    "def tratamento_pln(texto):\n",
    "    # normalização: tranforma o texto todo em minúsculo\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # mantém letras, acentos E NÚMEROS, mas remove pontuação\n",
    "    texto = re.sub(r'[^a-zA-Z0-9áéíóúâêôãõç\\s]', '', texto)\n",
    "\n",
    "    # tokenização com spacy\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # remoção de stopwords, pontuação e lematização\n",
    "    clean_tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if token.text not in stop_words and not token.is_punct\n",
    "    ]\n",
    "\n",
    "    # juntar tokens lematizados de volta em uma string\n",
    "    clean_text = \" \".join(clean_tokens)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# Visualizando as Stop Words\n",
    "print(\"Tamanho do conjunto stop_words:\", len(stop_words))\n",
    "print(\"\\nStop_words ordenadas:\\n\", sorted(list(stop_words)))\n",
    "\n",
    "# Chamada de PLN\n",
    "texto_pdf_tratado = tratamento_pln(texto_pdf)\n",
    "\n",
    "# Tamanho do texto original\n",
    "print(\"Tamanho do texto original em caracteres:\", len(texto_pdf))\n",
    "\n",
    "# Arquivo PDF tratado\n",
    "print(\"\\nTEXTO TRATADO:\\n\")\n",
    "print(texto_pdf_tratado)\n",
    "\n",
    "# Tamanho do texto tratado\n",
    "print(\"\\nTamanho do texto tratado em caracteres:\", len(texto_pdf_tratado))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking e Embedding\n",
    "\n",
    "- O embedding é o que lida com a vetorização, pega os Ids Numéricos da tokenização e os transforma em vetores, juntando o significado + contexto e armazenam no VectorDB\n",
    "    -Exemplo: o id 2 -> [0.2, 0.8, -0.1, ..., 0.5]\n",
    "\n",
    "# Questão 3.A\n",
    "\n",
    "    -Estou usando o RecursiveCharacter TextSplitter para quebrar o texto em chunks. O recursive chunk é um método recursivo que preserva ao máximo a estrutura semântica do texto, basicamente ele olha o tamanho do chunk que eu defino no código abaixo, se o chunk for maior que 200 ele tenta quebrar o texto usando o próximo separador que seria por linha, quando o chunk for menor que o tamanho definido o processo acaba. Escolhi o método recursivo buscando preservar o contexto do texto, assim não haverá tanta perca semântica.\n",
    "\n",
    "    -O tamanho do chunk escolhido foi de 200 caracteres, escolhi esse tamanho porque não quero perder o contexto e também quero que o meu chunk não seja tão grande para que ele me retorne apenas a resposta consolidada. Fiz a escolha do overlap de 50 caracteres, sendo mais que 10% do chunk original para que não haja a perca do contexto.\n",
    "\n",
    "# Questão 3.B\n",
    "\n",
    "    -Optei por fazer embedding de frase, usando o Sentence Transformer. Novamente meu objetivo é manter o contexto do texto, fazendo a vetorização por frases completas consigo fazer a diferença de frases como \"Fui ao banco da praça\" e \"Sentei no banco da praça\". Se eu escolhesse o embedding por palavras (Word Embeddings) a palavra banco teriam o mesmo vetor (significado) sem o contexto da frase.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks gerados:\n",
      " ['cientista analisar grande volume dado descobrir padrão poder prever comportamento humano embora resultado inicial promissor ainda haver fazer algum algoritmos aprender rapidamente enquanto outro', 'algoritmos aprender rapidamente enquanto outro exigir ajuste fino reprocessamento informação equipe perceber contexto desempenhar papel crucial interpretação dado coletar durante teste surgir desafio', 'dado coletar durante teste surgir desafio inesperado relacionar   ambiguidade linguístico   qualidade fonte utilizar frase simples poder assumir significado diferente depender domínio aplicar final', 'diferente depender domínio aplicar final primeiro fase projeto pesquisador confiante técnica inteligência artificial evoluir consciente limitação atual necessário combinar abordagem estatístico', 'atual necessário combinar abordagem estatístico conhecimento linguístico profundo']\n",
      "\n",
      "Quantidade de chunks: 5\n",
      "\n",
      "IDs gerados: ['doc_0', 'doc_1', 'doc_2', 'doc_3', 'doc_4']\n",
      "Quantidade de embeddings: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Preparação do Texto para BD Vetorial (chunks)\n",
    "\n",
    "# Criando o text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,      # tamanho máximo de cada chunk\n",
    "    chunk_overlap=50     # overlap para manter o contexto\n",
    ")\n",
    "\n",
    "# Aplica o chunk no texto limpo\n",
    "chunks = text_splitter.split_text(texto_pdf_tratado)\n",
    "\n",
    "print(\"Chunks gerados:\\n\", chunks)\n",
    "print(\"\\nQuantidade de chunks:\", len(chunks))\n",
    "\n",
    "# Criação do embedding (transforma chunks em vetores)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Converte lista de chunks em embeddings\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Gerando IDs automaticamente: doc_0, doc_1, doc_2...\n",
    "uids = [f\"doc_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "print(\"\\nIDs gerados:\", uids)\n",
    "print(\"Quantidade de embeddings:\", len(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aplicação do ChromaDB\n",
    "\n",
    "# Criar o banco de dados\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Criar a coleção (tipo uma tabela)\n",
    "collection = client.create_collection(name=\"machlrn\")\n",
    "# client.delete_collection(\"machlrn\")  # comando para apagar a coleção (caso queira resetar)\n",
    "\n",
    "# Adicionar os documentos ao banco de dados\n",
    "collection.add(\n",
    "    documents=chunks,        # lista de textos dos chunks\n",
    "    embeddings=embeddings,   # lista de vetores\n",
    "    ids=uids                 # lista de IDs\n",
    ")\n",
    "\n",
    "#print(\"Dados adicionados com sucesso ao ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Anotações sobre como é armazenado no banco\n",
    "\n",
    "    - Embeddings -> indíce de busca, meu banco pega minha query(pergunta) e compara com os vetores numéricos até achar o mais similar (só localiza)\n",
    "    - Documents -> é uma lista de chunks de texto, o banco pega a resposta mais similar e fornece esse conteúdo de volta\n",
    "    - Ids -> chave de ligação entre o embedding e documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui estou fazendo um guardrail de entrada\n",
    "# Consiste basicamente em analisar a pergunta do usuário e ver se ela tem relação com o documento.\n",
    "# Além disso posso bloquear palavras de ódio ou assuntos que não quero permitir.\n",
    "\n",
    "palavras_bloqueadas = ['comida', 'assassinato', 'futebol', 'política', 'morte']\n",
    "\n",
    "def query_guardrail(query_text):\n",
    "    \"\"\"Verifica se a pergunta contém palavras bloqueadas.\"\"\"\n",
    "    \n",
    "    query_lower = query_text.lower()  # transforma a frase para minúsculo\n",
    "    \n",
    "    for word in palavras_bloqueadas:  # percorre a lista de palavras bloqueadas\n",
    "        if word in query_lower:       # se a palavra estiver na pergunta\n",
    "            print(f\"Pergunta inválida. O tópico '{word}' não tem relação com o documento.\")\n",
    "            return False\n",
    "    \n",
    "    # se estiver tudo certo, aprova a pergunta\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta inválida. O tópico 'comida' não tem relação com o documento.\n",
      "Desculpe, só posso responder perguntas sobre o documento fornecido.\n"
     ]
    }
   ],
   "source": [
    "# Realizar a busca usando collection.query\n",
    "\n",
    "# Query 1\n",
    "# query_text = \"Quem foram os criados do GPS?\"  \n",
    "# Query 2\n",
    "# query_text = \"O que é agente?\"\n",
    "query_text = \"Qual a porcao necessaria de comida?\"  # teste do guardrail\n",
    "\n",
    "# aplicando o guardrail de entrada\n",
    "if query_guardrail(query_text):\n",
    "    print(\"Pergunta aprovada. Iniciando busca...\")\n",
    "\n",
    "    # transformar pergunta em embedding\n",
    "    query_embedding = model.encode([query_text])\n",
    "\n",
    "    # consultar o vetor no banco ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=1\n",
    "    )\n",
    "\n",
    "    print(results)  # resultado bruto\n",
    "\n",
    "else:\n",
    "    print(\"Desculpe, só posso responder perguntas sobre o documento fornecido.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se tiver info sensível no documento, o guardrail de recuperação não permite acessar\n",
    "\n",
    "palavras_sensiveis = ['confidencial', 'secreto', 'senha', 'salário']\n",
    "\n",
    "def document_guardrail(document_text, doc_id=\"Documento\"):\n",
    "    \"\"\"Verifica se o documento recuperado contém palavras sensíveis.\"\"\"\n",
    "    \n",
    "    doc_lower = document_text.lower()\n",
    "\n",
    "    for word in palavras_sensiveis:\n",
    "        if word in doc_lower:\n",
    "            print(f\"O documento '{doc_id}' foi bloqueado por conter informações sensíveis ({word}).\")\n",
    "            return False\n",
    "    \n",
    "    # se estiver tudo OK\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Função de similaridade do cosseno\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Função de distância euclidiana\n",
    "def euclidean_dist(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "# Gerar embedding do chunk retornado na busca\n",
    "chunk_result = model.encode([results['documents'][0][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc_0\n",
      "Distância: 1.2529549598693848\n",
      "Documento: cientista analisar grande volume dado descobrir padrão poder prever comportamento humano embora resultado inicial promissor ainda haver fazer algum algoritmos aprender rapidamente enquanto outro\n",
      "Cosseno: 0.37352252\n",
      "Euclidiana: 1.1193547\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Imprimir os resultados de forma organizada\n",
    "# Fazendo a varredura sobre os campos 'ids', 'distances' e 'documents'\n",
    "\n",
    "for i in range(len(results['ids'][0])):\n",
    "\n",
    "    doc_id = results['ids'][0][i]\n",
    "    distance = results['distances'][0][i]  # quanto menor, mais similar\n",
    "    document = results['documents'][0][i]  # texto do chunk encontrado\n",
    "\n",
    "    # Guardrail de recuperação\n",
    "    if document_guardrail(document):\n",
    "\n",
    "        print(f\"ID: {doc_id}\")\n",
    "        print(f\"Distância: {distance}\")\n",
    "        print(f\"Documento: {document}\")\n",
    "\n",
    "        # Gerar embedding do chunk atual\n",
    "        chunk_vector = model.encode([document])[0]\n",
    "\n",
    "        # Similaridades\n",
    "        print(\"Cosseno:\", cosine_sim(query_embedding[0], chunk_vector))\n",
    "        print(\"Euclidiana:\", euclidean_dist(query_embedding[0], chunk_vector))\n",
    "\n",
    "        print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
